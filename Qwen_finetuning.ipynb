{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8979fc",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# LoRA finetuning for Qwen 2.5 14B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92a3b3",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Start with setup of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65269f76-a952-4065-a175-663d95aafdf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Block torchvision from being imported\n",
    "sys.modules[\"torchvision\"] = None \n",
    "# Block tensorflow as well to avoid the numpy errors from earlier\n",
    "sys.modules[\"tensorflow\"] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a3f6161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import gc\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759e54",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## For Logging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4fe343c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mt-p-angevare\u001b[0m (\u001b[33mt-p-angevare-university-of-twente\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20260122_223729-d3psnd4w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/d3psnd4w' target=\"_blank\">14b-ioc-extraction</a></strong> to <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/d3psnd4w' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/d3psnd4w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/d3psnd4w?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdc68a100d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen-fine-tuning\", name=\"14b-ioc-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360cf854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bd301",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Usage of AI4privacy dataset cleaning and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18387680",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "https://huggingface.co/datasets/ai4privacy/pii-masking-300k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97352ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 29908\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 7946\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ai4privacy/pii-masking-300k\")\n",
    "dataset = dataset.filter(lambda x: x['language'] == 'English')\n",
    "dataset = dataset.select_columns([\"source_text\", \"privacy_mask\", \"id\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb6ee38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_entity_mapping = {\n",
    "    'EMAIL' : 'EMAIL',\n",
    "    'LASTNAME1' : 'PERSON',\n",
    "    'IP' : 'IP',\n",
    "    'GIVENNAME1' : 'PERSON',\n",
    "    'TEL' : 'PHONE',\n",
    "    'CITY' : 'LOCATION',\n",
    "    'STATE' : 'LOCATION',\n",
    "    'COUNTRY' : 'LOCATION',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04564bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_entities(privacy_mask, source_text):\n",
    "    \"\"\"\n",
    "    Clean and combine entities, merging consecutive PERSON entities into full names.\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    for ent in privacy_mask:\n",
    "        if ent['label'] in dataset_entity_mapping.keys():\n",
    "            entities.append({\n",
    "                'type': dataset_entity_mapping.get(ent['label']),\n",
    "                'text': ent['value'],\n",
    "                'start_pos': ent['start'],\n",
    "                'end_pos': ent['end'],\n",
    "                'original_label': ent['label']\n",
    "            })\n",
    "    \n",
    " \n",
    "    entities.sort(key=lambda x: x['start_pos'])\n",
    "\n",
    "    merged_entities = []\n",
    "    i = 0\n",
    "    while i < len(entities):\n",
    "        current = entities[i]\n",
    "        \n",
    "        if current['type'] == 'PERSON' and i + 1 < len(entities):\n",
    "            next_ent = entities[i + 1]\n",
    "            \n",
    "            if (next_ent['type'] == 'PERSON' and \n",
    "                next_ent['start_pos'] - current['end_pos'] <= 3):\n",
    "\n",
    "                is_first_given = 'GIVENNAME' in current['original_label']\n",
    "                is_second_last = 'LASTNAME' in next_ent['original_label']\n",
    "                is_first_last = 'LASTNAME' in current['original_label']\n",
    "                is_second_given = 'GIVENNAME' in next_ent['original_label']\n",
    "                \n",
    "                if (is_first_given and is_second_last) or (is_first_last and is_second_given):\n",
    "                    # Merge into full name\n",
    "                    full_name = source_text[current['start_pos']:next_ent['end_pos']]\n",
    "                    merged_entities.append({\n",
    "                        'entity': full_name,\n",
    "                        'type': 'PERSON',\n",
    "                    })\n",
    "                    i += 2 \n",
    "                    continue\n",
    "        \n",
    "  \n",
    "        merged_entities.append({\n",
    "            'entity': current['text'],\n",
    "            'type': current['type'],\n",
    "\n",
    "        })\n",
    "        i += 1\n",
    "    \n",
    "    return merged_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f4e32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {'privacy_mask': clean_entities(x['privacy_mask'], x['source_text'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81dc3f4",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Set up LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56659c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is to extract any entity from the input text. For each entity found you MUST indicate the type in UPPERCASE. ONLY extract entities if literal entity is present in input text.\n",
    "The expected entity types are the following: EMAIL, IP, PERSON, LOCATION, PHONE\n",
    "\n",
    "The output MUST be in a JSON object with key 'entities' and the value a list of dictionaries including every entity found. For each entity you MUST indicate the type in UPPERCASE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e45961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-14B\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d907eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_chatml(source_text, privacy_mask):\n",
    "    # Convert to the JSON format expected by the prompt\n",
    "    entities = []\n",
    "    for ent in privacy_mask:\n",
    "        entities.append({\"entity\": ent['entity'], \"type\": ent['type']})\n",
    "    \n",
    "    entities_json = {\n",
    "        \"entities\": entities\n",
    "    }\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": source_text},\n",
    "        {\"role\": \"assistant\", \"content\": json.dumps(entities_json, indent=2)}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2936c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.map(lambda x: {\"text\": convert_to_chatml(x['source_text'], x['privacy_mask'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b045da8-7031-474c-bbc9-59a415bad8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     11096\n",
      "1      4383\n",
      "2      4306\n",
      "3      3275\n",
      "4      2486\n",
      "5      1569\n",
      "6      1071\n",
      "7       561\n",
      "8       428\n",
      "9       275\n",
      "10      153\n",
      "11       91\n",
      "12       79\n",
      "13       25\n",
      "14       39\n",
      "15       31\n",
      "16       12\n",
      "17        6\n",
      "18       10\n",
      "19        5\n",
      "20        4\n",
      "22        1\n",
      "24        1\n",
      "26        1\n",
      "Name: count, dtype: int64\n",
      "634     1\n",
      "643     1\n",
      "655     1\n",
      "678     1\n",
      "698     1\n",
      "       ..\n",
      "2336    1\n",
      "2342    1\n",
      "2411    1\n",
      "2692    1\n",
      "2713    1\n",
      "Name: count, Length: 1128, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_lengths = pd.Series([len(lst) for lst in dataset['train']['privacy_mask']])\n",
    "token_lengths = pd.Series([len(lst) for lst in dataset['train']['text']])\n",
    "\n",
    "# Print distribution\n",
    "print(df_lengths.value_counts().sort_index())\n",
    "print(token_lengths.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60e84243-004e-411c-b226-4b1610311b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_none = dataset.filter(lambda x: len(x['privacy_mask']) == 0 and len(x['text']) <= 2048)\n",
    "dataset_low = dataset.filter(lambda x: len(x['privacy_mask']) >= 4 and len(x['privacy_mask']) <= 6 and len(x['text']) <= 2048)\n",
    "dataset_high = dataset.filter(lambda x: len(x['privacy_mask']) < 4 and len(x['privacy_mask']) > 0 and len(x['text']) <= 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e06b1aea-6f1c-43ce-929c-2319978bcb76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11964\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_high['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17bde313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1275\n",
      "Validation samples: 300\n"
     ]
    }
   ],
   "source": [
    "train = concatenate_datasets([dataset_high['train'].select(range(600)),dataset_low['train'].select(range(450)), dataset_none['train'].select(range(225))])\n",
    "val = concatenate_datasets([dataset_high['validation'].select(range(180)),dataset_low['validation'].select(range(90)), dataset_none['validation'].select(range(30))])\n",
    "\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Validation samples: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b196bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf51c16d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b10eb70f8f844d5a9d8ff756cec9053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ad7848",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "alpha = 2*r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e195b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                   \n",
    "    lora_alpha=16,           \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dce7b5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_qwen_14b_output\",\n",
    "\n",
    "    num_train_epochs=1,                 \n",
    "    gradient_checkpointing=True,\n",
    "    max_length=2048,\n",
    "    per_device_train_batch_size=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    \n",
    "\n",
    "    learning_rate=2e-4,                  \n",
    "    warmup_ratio=0.1,                  \n",
    "    \n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,                      \n",
    "    \n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    packing=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"qwen-14b-pii\",\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a9dec7-f319-4ed5-aa20-95d32a27aa5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=2,\n",
    "            early_stopping_threshold=0.005\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a60ea7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1275 2:38:19 < 43:37, 0.11 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.088400</td>\n",
       "      <td>1.112496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.126100</td>\n",
       "      <td>1.028311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.008400</td>\n",
       "      <td>0.998619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.953500</td>\n",
       "      <td>0.980811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>0.988606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.874200</td>\n",
       "      <td>0.963838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.944100</td>\n",
       "      <td>0.952565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.753300</td>\n",
       "      <td>0.942609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.868200</td>\n",
       "      <td>0.938489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.936300</td>\n",
       "      <td>0.933728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.0412765340805055, metrics={'train_runtime': 9505.5441, 'train_samples_per_second': 0.134, 'train_steps_per_second': 0.134, 'total_flos': 2.709128071887667e+16, 'train_loss': 1.0412765340805055})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2417ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./sft_qwen_14b_output/final_model\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/entropy</td><td>█▆▅▃▃▁▂▂▁▁</td></tr><tr><td>eval/loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr><tr><td>eval/mean_token_accuracy</td><td>▁▄▅▆▅▇▇▇██</td></tr><tr><td>eval/num_tokens</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>eval/runtime</td><td>▆▄▅█▇▃▃▁▂▄</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁▁▁████▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy</td><td>▇█▇▃▄▃▂▃▂▃▂▂▂▂▃▂▂▂▃▂▂▂▂▃▃▂▂▂▂▁▂▂▂▁▂▃▂▂▂▂</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>+5</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/entropy</td><td>0.94252</td></tr><tr><td>eval/loss</td><td>0.93373</td></tr><tr><td>eval/mean_token_accuracy</td><td>0.7898</td></tr><tr><td>eval/num_tokens</td><td>322422</td></tr><tr><td>eval/runtime</td><td>400.3402</td></tr><tr><td>eval/samples_per_second</td><td>0.749</td></tr><tr><td>eval/steps_per_second</td><td>0.095</td></tr><tr><td>total_flos</td><td>2.709128071887667e+16</td></tr><tr><td>train/entropy</td><td>0.93633</td></tr><tr><td>train/epoch</td><td>0.78431</td></tr><tr><td>+10</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">14b-ioc-extraction</strong> at: <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/d3psnd4w' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/d3psnd4w</a><br> View project at: <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260122_223729-d3psnd4w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./sft_qwen_14b_output/final_model\")\n",
    "print(\"Model saved to ./sft_qwen_14b_output/final_model\")\n",
    "\n",
    "# Log final metrics\n",
    "if wandb.run:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bd00623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    eval/entropy  eval/samples_per_second  _step  train/epoch  train/entropy  \\\n",
      "0            NaN                      NaN      0       0.0320       1.958536   \n",
      "1            NaN                      NaN      1       0.0640       1.958171   \n",
      "2            NaN                      NaN      2       0.0960       1.957968   \n",
      "3            NaN                      NaN      3       0.1280       1.957781   \n",
      "4            NaN                      NaN      4       0.1600       1.956328   \n",
      "5       1.958243                    0.606      5       0.1600            NaN   \n",
      "6            NaN                      NaN      6       0.1920       1.963717   \n",
      "7            NaN                      NaN      7       0.2240       2.016932   \n",
      "8            NaN                      NaN      8       0.2560       2.084045   \n",
      "9            NaN                      NaN      9       0.2880       2.094191   \n",
      "10           NaN                      NaN     10       0.3200       2.071899   \n",
      "11      2.058485                    0.610     11       0.3200            NaN   \n",
      "12           NaN                      NaN     12       0.3520       2.041616   \n",
      "13           NaN                      NaN     13       0.3840       1.995839   \n",
      "14           NaN                      NaN     14       0.4160       1.924913   \n",
      "15           NaN                      NaN     15       0.4480       1.788786   \n",
      "16           NaN                      NaN     16       0.4800       1.630296   \n",
      "17      1.553811                    0.610     17       0.4800            NaN   \n",
      "18           NaN                      NaN     18       0.5120       1.482840   \n",
      "19           NaN                      NaN     19       0.5440       1.313624   \n",
      "20           NaN                      NaN     20       0.5760       1.144133   \n",
      "21           NaN                      NaN     21       0.6080       0.954084   \n",
      "22           NaN                      NaN     22       0.6400       0.733966   \n",
      "23      0.602679                    0.610     23       0.6400            NaN   \n",
      "24           NaN                      NaN     24       0.6720       0.519782   \n",
      "25           NaN                      NaN     25       0.7040       0.342780   \n",
      "26           NaN                      NaN     26       0.7360       0.181405   \n",
      "27           NaN                      NaN     27       0.7680       0.081144   \n",
      "28           NaN                      NaN     28       0.8000       0.042711   \n",
      "29      0.034833                    0.609     29       0.8000            NaN   \n",
      "30           NaN                      NaN     30       0.8320       0.032575   \n",
      "31           NaN                      NaN     31       0.8640       0.029895   \n",
      "32           NaN                      NaN     32       0.8960       0.028767   \n",
      "33           NaN                      NaN     33       0.9280       0.028282   \n",
      "34           NaN                      NaN     34       0.9600       0.027889   \n",
      "35      0.027035                    0.605     35       0.9600            NaN   \n",
      "36           NaN                      NaN     36       0.9920       0.027752   \n",
      "37           NaN                      NaN     37       1.0224       0.027470   \n",
      "38           NaN                      NaN     38       1.0544       0.027178   \n",
      "39           NaN                      NaN     39       1.0864       0.027190   \n",
      "40           NaN                      NaN     40       1.1184       0.027209   \n",
      "41      0.026362                    0.604     41       1.1184            NaN   \n",
      "42           NaN                      NaN     42       1.1184            NaN   \n",
      "\n",
      "    eval/steps_per_second  train/loss    _runtime  eval/runtime  \\\n",
      "0                     NaN      2.1611  786.563118           NaN   \n",
      "1                     NaN      2.1598  786.563118           NaN   \n",
      "2                     NaN      2.1558  786.563118           NaN   \n",
      "3                     NaN      2.1475  786.563118           NaN   \n",
      "4                     NaN      2.1315  786.563118           NaN   \n",
      "5                   0.076         NaN  786.563118      824.5038   \n",
      "6                     NaN      2.1021  786.563118           NaN   \n",
      "7                     NaN      2.0671  786.563118           NaN   \n",
      "8                     NaN      2.0409  786.563118           NaN   \n",
      "9                     NaN      2.0135  786.563118           NaN   \n",
      "10                    NaN      1.9718  786.563118           NaN   \n",
      "11                  0.077         NaN  786.563118      819.3431   \n",
      "12                    NaN      1.9188  786.563118           NaN   \n",
      "13                    NaN      1.8527  786.563118           NaN   \n",
      "14                    NaN      1.7712  786.563118           NaN   \n",
      "15                    NaN      1.6675  786.563118           NaN   \n",
      "16                    NaN      1.5434  786.563118           NaN   \n",
      "17                  0.077         NaN  786.563118      820.0341   \n",
      "18                    NaN      1.3989  786.563118           NaN   \n",
      "19                    NaN      1.2289  786.563118           NaN   \n",
      "20                    NaN      1.0360  786.563118           NaN   \n",
      "21                    NaN      0.8215  786.563118           NaN   \n",
      "22                    NaN      0.5826  786.563118           NaN   \n",
      "23                  0.077         NaN  786.563118      820.2847   \n",
      "24                    NaN      0.3598  786.563118           NaN   \n",
      "25                    NaN      0.1819  786.563118           NaN   \n",
      "26                    NaN      0.0666  786.563118           NaN   \n",
      "27                    NaN      0.0185  786.563118           NaN   \n",
      "28                    NaN      0.0065  786.563118           NaN   \n",
      "29                  0.077         NaN  786.563118      820.5675   \n",
      "30                    NaN      0.0045  786.563118           NaN   \n",
      "31                    NaN      0.0041  786.563118           NaN   \n",
      "32                    NaN      0.0039  786.563118           NaN   \n",
      "33                    NaN      0.0038  786.563118           NaN   \n",
      "34                    NaN      0.0037  786.563118           NaN   \n",
      "35                  0.076         NaN  786.563118      827.0299   \n",
      "36                    NaN      0.0037  786.563118           NaN   \n",
      "37                    NaN      0.0037  786.563118           NaN   \n",
      "38                    NaN      0.0036  786.563118           NaN   \n",
      "39                    NaN      0.0036  786.563118           NaN   \n",
      "40                    NaN      0.0036  786.563118           NaN   \n",
      "41                  0.076         NaN  786.563118      827.7177   \n",
      "42                    NaN         NaN  786.563118           NaN   \n",
      "\n",
      "    train/grad_norm  train/learning_rate  eval/loss    _timestamp  \\\n",
      "0          0.960938         9.574468e-07        NaN  1.768229e+09   \n",
      "1          0.972656         2.021277e-06        NaN  1.768230e+09   \n",
      "2          0.957031         3.085106e-06        NaN  1.768231e+09   \n",
      "3          0.976562         4.148936e-06        NaN  1.768232e+09   \n",
      "4          0.960938         5.212766e-06        NaN  1.768234e+09   \n",
      "5               NaN                  NaN   2.115630  1.768234e+09   \n",
      "6          0.789062         6.276596e-06        NaN  1.768236e+09   \n",
      "7          0.349609         7.340426e-06        NaN  1.768237e+09   \n",
      "8          0.287109         8.404255e-06        NaN  1.768238e+09   \n",
      "9          0.283203         9.468085e-06        NaN  1.768239e+09   \n",
      "10         0.300781         9.999136e-06        NaN  1.768241e+09   \n",
      "11              NaN                  NaN   1.944338  1.768241e+09   \n",
      "12         0.333984         9.992227e-06        NaN  1.768243e+09   \n",
      "13         0.388672         9.978418e-06        NaN  1.768244e+09   \n",
      "14         0.468750         9.957728e-06        NaN  1.768245e+09   \n",
      "15         0.558594         9.930187e-06        NaN  1.768246e+09   \n",
      "16         0.656250         9.895831e-06        NaN  1.768247e+09   \n",
      "17              NaN                  NaN   1.468676  1.768248e+09   \n",
      "18         0.808594         9.854709e-06        NaN  1.768249e+09   \n",
      "19         0.945312         9.806877e-06        NaN  1.768251e+09   \n",
      "20         1.101562         9.752402e-06        NaN  1.768252e+09   \n",
      "21         1.328125         9.691359e-06        NaN  1.768253e+09   \n",
      "22         1.320312         9.623831e-06        NaN  1.768254e+09   \n",
      "23              NaN                  NaN   0.451372  1.768255e+09   \n",
      "24         1.187500         9.549913e-06        NaN  1.768256e+09   \n",
      "25         0.824219         9.469707e-06        NaN  1.768257e+09   \n",
      "26         0.455078         9.383323e-06        NaN  1.768258e+09   \n",
      "27         0.179688         9.290881e-06        NaN  1.768260e+09   \n",
      "28         0.056641         9.192509e-06        NaN  1.768261e+09   \n",
      "29              NaN                  NaN   0.004920  1.768262e+09   \n",
      "30         0.038574         9.088342e-06        NaN  1.768263e+09   \n",
      "31         0.028564         8.978525e-06        NaN  1.768264e+09   \n",
      "32         0.024780         8.863209e-06        NaN  1.768265e+09   \n",
      "33         0.024902         8.742554e-06        NaN  1.768266e+09   \n",
      "34         0.023438         8.616726e-06        NaN  1.768268e+09   \n",
      "35              NaN                  NaN   0.003593  1.768268e+09   \n",
      "36         0.021484         8.485900e-06        NaN  1.768270e+09   \n",
      "37         0.020996         8.350255e-06        NaN  1.768271e+09   \n",
      "38         0.020630         8.209981e-06        NaN  1.768272e+09   \n",
      "39         0.020752         8.065270e-06        NaN  1.768273e+09   \n",
      "40         0.019409         7.916322e-06        NaN  1.768274e+09   \n",
      "41              NaN                  NaN   0.003484  1.768275e+09   \n",
      "42              NaN                  NaN        NaN  1.768275e+09   \n",
      "\n",
      "    train/mean_token_accuracy  train/num_tokens  train/global_step  \\\n",
      "0                    0.569618           81920.0                 10   \n",
      "1                    0.568897          163840.0                 20   \n",
      "2                    0.569313          245760.0                 30   \n",
      "3                    0.569129          327680.0                 40   \n",
      "4                    0.572187          409600.0                 50   \n",
      "5                         NaN               NaN                 50   \n",
      "6                    0.573716          491520.0                 60   \n",
      "7                    0.573141          573440.0                 70   \n",
      "8                    0.571404          655360.0                 80   \n",
      "9                    0.570780          737280.0                 90   \n",
      "10                   0.576492          819200.0                100   \n",
      "11                        NaN               NaN                100   \n",
      "12                   0.583867          901120.0                110   \n",
      "13                   0.595682          983040.0                120   \n",
      "14                   0.608207         1064960.0                130   \n",
      "15                   0.614298         1146880.0                140   \n",
      "16                   0.618334         1228800.0                150   \n",
      "17                        NaN               NaN                150   \n",
      "18                   0.642882         1310720.0                160   \n",
      "19                   0.690607         1392640.0                170   \n",
      "20                   0.745536         1474560.0                180   \n",
      "21                   0.800954         1556480.0                190   \n",
      "22                   0.859283         1638400.0                200   \n",
      "23                        NaN               NaN                200   \n",
      "24                   0.914934         1720320.0                210   \n",
      "25                   0.962133         1802240.0                220   \n",
      "26                   0.987378         1884160.0                230   \n",
      "27                   0.997713         1966080.0                240   \n",
      "28                   1.000000         2048000.0                250   \n",
      "29                        NaN               NaN                250   \n",
      "30                   1.000000         2129920.0                260   \n",
      "31                   1.000000         2211840.0                270   \n",
      "32                   1.000000         2293760.0                280   \n",
      "33                   1.000000         2375680.0                290   \n",
      "34                   1.000000         2457600.0                300   \n",
      "35                        NaN               NaN                300   \n",
      "36                   1.000000         2539520.0                310   \n",
      "37                   1.000000         2617344.0                320   \n",
      "38                   1.000000         2699264.0                330   \n",
      "39                   1.000000         2781184.0                340   \n",
      "40                   1.000000         2863104.0                350   \n",
      "41                        NaN               NaN                350   \n",
      "42                        NaN               NaN                350   \n",
      "\n",
      "    eval/mean_token_accuracy  eval/num_tokens  \n",
      "0                        NaN              NaN  \n",
      "1                        NaN              NaN  \n",
      "2                        NaN              NaN  \n",
      "3                        NaN              NaN  \n",
      "4                        NaN              NaN  \n",
      "5                   0.577299         409600.0  \n",
      "6                        NaN              NaN  \n",
      "7                        NaN              NaN  \n",
      "8                        NaN              NaN  \n",
      "9                        NaN              NaN  \n",
      "10                       NaN              NaN  \n",
      "11                  0.587022         819200.0  \n",
      "12                       NaN              NaN  \n",
      "13                       NaN              NaN  \n",
      "14                       NaN              NaN  \n",
      "15                       NaN              NaN  \n",
      "16                       NaN              NaN  \n",
      "17                  0.628149        1228800.0  \n",
      "18                       NaN              NaN  \n",
      "19                       NaN              NaN  \n",
      "20                       NaN              NaN  \n",
      "21                       NaN              NaN  \n",
      "22                       NaN              NaN  \n",
      "23                  0.892368        1638400.0  \n",
      "24                       NaN              NaN  \n",
      "25                       NaN              NaN  \n",
      "26                       NaN              NaN  \n",
      "27                       NaN              NaN  \n",
      "28                       NaN              NaN  \n",
      "29                  1.000000        2048000.0  \n",
      "30                       NaN              NaN  \n",
      "31                       NaN              NaN  \n",
      "32                       NaN              NaN  \n",
      "33                       NaN              NaN  \n",
      "34                       NaN              NaN  \n",
      "35                  1.000000        2457600.0  \n",
      "36                       NaN              NaN  \n",
      "37                       NaN              NaN  \n",
      "38                       NaN              NaN  \n",
      "39                       NaN              NaN  \n",
      "40                       NaN              NaN  \n",
      "41                  1.000000        2863104.0  \n",
      "42                       NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(\"/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/27s35zzj\")\n",
    "history = run.history()\n",
    "print(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
