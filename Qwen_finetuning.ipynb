{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a3f6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import gc\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4fe343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mt-p-angevare\u001b[0m (\u001b[33mt-p-angevare-university-of-twente\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20260113_223007-tq9178i3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/tq9178i3' target=\"_blank\">14b-ioc-extraction</a></strong> to <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/tq9178i3' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/tq9178i3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/tq9178i3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fe775a136d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Qwen-fine-tuning\", name=\"14b-ioc-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "360cf854",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97352ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 29908\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 7946\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ai4privacy/pii-masking-300k\")\n",
    "dataset = dataset.filter(lambda x: x['language'] == 'English')\n",
    "dataset = dataset.select_columns([\"source_text\", \"privacy_mask\", \"id\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ceb6ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_entity_mapping = {\n",
    "    'USERNAME' : 'USERNAME',\n",
    "    'EMAIL' : 'EMAIL',\n",
    "    'LASTNAME1' : 'PERSON',\n",
    "    'IP' : 'IP',\n",
    "    'GIVENNAME1' : 'PERSON',\n",
    "    'TEL' : 'PHONE',\n",
    "    'CITY' : 'LOCATION',\n",
    "    'POSTCODE' : 'LOCATION',\n",
    "    'STREET': 'LOCATION',\n",
    "    'STATE' : 'LOCATION',\n",
    "    'BUILDING' : 'LOCATION',\n",
    "    'COUNTRY' : 'LOCATION',\n",
    "    'SECADDRESS' : 'LOCATION',\n",
    "    'LASTNAME2' : 'PERSON',\n",
    "    'GIVENNAME2' : 'PERSON',\n",
    "    'GEOCOORD' : 'LOCATION',\n",
    "    'LASTNAME3' : 'PERSON'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04564bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entities(privacy_mask):\n",
    "    new_entities = []\n",
    "    for ent in privacy_mask:\n",
    "        if ent['label'] in dataset_entity_mapping.keys():\n",
    "            new_entities.append({\n",
    "                'type': dataset_entity_mapping.get(ent['label']),\n",
    "                'text' : ent['value'],\n",
    "                'start_pos' : ent['start'],\n",
    "                'end_pos' : ent['end']\n",
    "            })\n",
    "    return new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60f4e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {'privacy_mask': clean_entities(x['privacy_mask'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e319409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': 'Subject: Group Messaging for Admissions Process\\n\\nGood morning, everyone,\\n\\nI hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\\n\\n- wynqvrh053 - Meeting at 10:20am\\n- luka.burg - Meeting at 21\\n- qahil.wittauer - Meeting at quarter past 13\\n- gholamhossein.ruschke - Meeting at 9:47 PM\\n- pdmjrsyoz1460 ',\n",
       " 'privacy_mask': [{'end_pos': 297,\n",
       "   'start_pos': 287,\n",
       "   'text': 'wynqvrh053',\n",
       "   'type': 'USERNAME'},\n",
       "  {'end_pos': 330, 'start_pos': 321, 'text': 'luka.burg', 'type': 'USERNAME'},\n",
       "  {'end_pos': 363,\n",
       "   'start_pos': 349,\n",
       "   'text': 'qahil.wittauer',\n",
       "   'type': 'USERNAME'},\n",
       "  {'end_pos': 416,\n",
       "   'start_pos': 395,\n",
       "   'text': 'gholamhossein.ruschke',\n",
       "   'type': 'USERNAME'},\n",
       "  {'end_pos': 453,\n",
       "   'start_pos': 440,\n",
       "   'text': 'pdmjrsyoz1460',\n",
       "   'type': 'USERNAME'}],\n",
       " 'id': '40767A'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d56659c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a cyber intelligence analyst with 20 years of experience in the the field.\n",
    "\n",
    "Your task is to extract any entity from the input text. For each entity found you MUST indicate the type in UPPERCASE. ONLY extract entities if literal entity is present in input text.\n",
    "The expected entity types are the following:\n",
    "\n",
    "- EMAIL: email addresses format (user@domain.tld)\n",
    "- IP: IP addresses (IPv4 x.x.x.x or IPv6)\n",
    "- BTC: ONLY Bitcoin wallet addresses (26-35 alphanumeric, starting with 1, 3, or bc1) EXCLUDE the word bitcoin or values (for example 2.0 BTC)\n",
    "- IBAN: iban bank account number\n",
    "- PERSON: Human names (John Smith, John, Catalina) EXCLUDE initials (for example A.H.) \n",
    "- LOCATION: cities, countries, geographic locations, regions\n",
    "- PHONE: phone numbers in any format\n",
    "- WEB_RESOURCE: URLs and web addresses EXCLUDE filenames\n",
    " \n",
    "**Output**:\n",
    "The output MUST be in a JSON object with key 'entities' and the value a list of dictionaries including every entity found. For each entity you MUST indicate the type in UPPERCASE.\n",
    "\n",
    "**OUTPUT EXAMPLE**:\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"entity\": \"<extracted text>\", \"type\": \"<TYPE>\"},\n",
    "    {\"entity\": \"<extracted text>\", \"type\": \"<TYPE>\"},\n",
    "  ]\n",
    "}\n",
    "\n",
    "Return empty array if no entities found in the input text.\n",
    "PAY ATTENTION to sentences that begin with entity type PERSON, for example Anna.\n",
    "PAY ATTENTION to when the sentences begin with possesive forms of entity type PERSON, for example Catalina's\n",
    "PAY ATTENTION to when the sentences contain a FULL NAME, the FULL NAME MUST be extracted as ONE entity.\n",
    "DO NOT include any entities from the example or the system prompt in your answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17e45961",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-14B\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d907eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_chatml(source_text, privacy_mask):\n",
    "    # Convert to the JSON format expected by the prompt\n",
    "    entities_json = {\n",
    "        \"entities\": [\n",
    "            {\"entity\": ent['text'], \"type\": ent['type']} \n",
    "            for ent in privacy_mask\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": source_text},\n",
    "        {\"role\": \"assistant\", \"content\": json.dumps(entities_json, indent=2)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2936c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d269d5249544479bea77074f0ef9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/29908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa35b91bc5d492bb136e7a1e1e57276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7946 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset.map(lambda x: {\"messages\": convert_to_chatml(x['source_text'], x['privacy_mask'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17bde313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Validation samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Reduced dataset for faster training (~2-3 hours instead of 12+)\n",
    "train = dataset['train'].select(range(5000))  # 5k samples (was 30k)\n",
    "val = dataset['validation'].select(range(500))\n",
    "\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Validation samples: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eb2a0ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Text (truncated) ===\n",
      "Subject: Group Messaging for Admissions Process\n",
      "\n",
      "Good morning, everyone,\n",
      "\n",
      "I hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developm...\n",
      "\n",
      "=== User Message (prompt + input) ===\n",
      "\n",
      "You are a cyber intelligence analyst with 20 years of experience in the the field.\n",
      "\n",
      "Your task is to extract any entity from the input text. For each entity found you MUST indicate the type in UPPERCASE. ONLY extract entities if literal entity is present in input text.\n",
      "The expected entity types are ...\n",
      "\n",
      "=== Assistant Response (with thinking tokens) ===\n",
      "Subject: Group Messaging for Admissions Process\n",
      "\n",
      "Good morning, everyone,\n",
      "\n",
      "I hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\n",
      "\n",
      "- wynqvrh053 - Meeting at 10:20am\n",
      "- luka.burg - Meeting at 21\n",
      "- qahil.wittauer - Meeting at quarter past 13\n",
      "- gholamhossein.ruschke - Meeting at 9:47 PM\n",
      "- pdmjrsyoz1460 ...\n",
      "\n",
      "=== Format: <think>reasoning</think> + JSON output ===\n"
     ]
    }
   ],
   "source": [
    "# verify format with thinking tokens\n",
    "example = train[0]\n",
    "print(\"=== Source Text (truncated) ===\")\n",
    "print(example['source_text'][:200] + \"...\")\n",
    "print(\"\\n=== User Message (prompt + input) ===\")\n",
    "print(example['messages'][0]['content'][:300] + \"...\")\n",
    "print(\"\\n=== Assistant Response (with thinking tokens) ===\")\n",
    "print(example['messages'][1]['content'][:500] + \"...\")\n",
    "print(\"\\n=== Format: <think>reasoning</think> + JSON output ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9b196bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf51c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3531e3fd10d441d88443f491438ba8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67e195b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                   \n",
    "    lora_alpha=16,           \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dce7b5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a48cc450af4727880bb0b6411a3b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0c5b352dec4ceab7baba93c2b33b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a183c806e0046a7b18b73148e83d71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ccc80985644924b4819bdeb1746387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration for DeepSeek-R1-Distill-Qwen-14B:\n",
      "  - Train samples: 5000\n",
      "  - Epochs: 3\n",
      "  - Effective batch size: 16\n",
      "  - Learning rate: 1e-05\n",
      "  - Eval every 50 steps\n",
      "  - Max grad norm: 0.5\n",
      "  - Early stopping patience: 2 evals\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_qwen_14b_output\",\n",
    "\n",
    "    num_train_epochs=3,                 \n",
    "    \n",
    "    max_length=512,\n",
    "    per_device_train_batch_size=1,       \n",
    "    gradient_accumulation_steps=16,      \n",
    "    \n",
    "\n",
    "    learning_rate=1e-5,                  \n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.5,                   \n",
    "    \n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,                      \n",
    "    \n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "\n",
    "    packing=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"qwen-14b-pii\",\n",
    "    bf16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=2,\n",
    "            early_stopping_threshold=0.005\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Training configuration for DeepSeek-R1-Distill-Qwen-14B:\")\n",
    "print(f\"  - Train samples: {len(train)}\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Eval every {training_args.eval_steps} steps\")\n",
    "print(f\"  - Max grad norm: {training_args.max_grad_norm}\")\n",
    "print(f\"  - Early stopping patience: 2 evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ea7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/939 5:58:02 < 31:18:33, 0.01 it/s, Epoch 0.48/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.457100</td>\n",
       "      <td>2.436293</td>\n",
       "      <td>2.180154</td>\n",
       "      <td>405461.000000</td>\n",
       "      <td>0.525159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.319400</td>\n",
       "      <td>2.291337</td>\n",
       "      <td>2.282052</td>\n",
       "      <td>810633.000000</td>\n",
       "      <td>0.523398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.937300</td>\n",
       "      <td>1.859043</td>\n",
       "      <td>1.862807</td>\n",
       "      <td>1216517.000000</td>\n",
       "      <td>0.583443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f2417ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./sft_qwen_14b_output/final_model\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./sft_qwen_14b_output/final_model\")\n",
    "print(\"Model saved to ./sft_qwen_14b_output/final_model\")\n",
    "\n",
    "# Log final metrics\n",
    "if wandb.run:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bd00623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    eval/entropy  eval/samples_per_second  _step  train/epoch  train/entropy  \\\n",
      "0            NaN                      NaN      0       0.0320       1.958536   \n",
      "1            NaN                      NaN      1       0.0640       1.958171   \n",
      "2            NaN                      NaN      2       0.0960       1.957968   \n",
      "3            NaN                      NaN      3       0.1280       1.957781   \n",
      "4            NaN                      NaN      4       0.1600       1.956328   \n",
      "5       1.958243                    0.606      5       0.1600            NaN   \n",
      "6            NaN                      NaN      6       0.1920       1.963717   \n",
      "7            NaN                      NaN      7       0.2240       2.016932   \n",
      "8            NaN                      NaN      8       0.2560       2.084045   \n",
      "9            NaN                      NaN      9       0.2880       2.094191   \n",
      "10           NaN                      NaN     10       0.3200       2.071899   \n",
      "11      2.058485                    0.610     11       0.3200            NaN   \n",
      "12           NaN                      NaN     12       0.3520       2.041616   \n",
      "13           NaN                      NaN     13       0.3840       1.995839   \n",
      "14           NaN                      NaN     14       0.4160       1.924913   \n",
      "15           NaN                      NaN     15       0.4480       1.788786   \n",
      "16           NaN                      NaN     16       0.4800       1.630296   \n",
      "17      1.553811                    0.610     17       0.4800            NaN   \n",
      "18           NaN                      NaN     18       0.5120       1.482840   \n",
      "19           NaN                      NaN     19       0.5440       1.313624   \n",
      "20           NaN                      NaN     20       0.5760       1.144133   \n",
      "21           NaN                      NaN     21       0.6080       0.954084   \n",
      "22           NaN                      NaN     22       0.6400       0.733966   \n",
      "23      0.602679                    0.610     23       0.6400            NaN   \n",
      "24           NaN                      NaN     24       0.6720       0.519782   \n",
      "25           NaN                      NaN     25       0.7040       0.342780   \n",
      "26           NaN                      NaN     26       0.7360       0.181405   \n",
      "27           NaN                      NaN     27       0.7680       0.081144   \n",
      "28           NaN                      NaN     28       0.8000       0.042711   \n",
      "29      0.034833                    0.609     29       0.8000            NaN   \n",
      "30           NaN                      NaN     30       0.8320       0.032575   \n",
      "31           NaN                      NaN     31       0.8640       0.029895   \n",
      "32           NaN                      NaN     32       0.8960       0.028767   \n",
      "33           NaN                      NaN     33       0.9280       0.028282   \n",
      "34           NaN                      NaN     34       0.9600       0.027889   \n",
      "35      0.027035                    0.605     35       0.9600            NaN   \n",
      "36           NaN                      NaN     36       0.9920       0.027752   \n",
      "37           NaN                      NaN     37       1.0224       0.027470   \n",
      "38           NaN                      NaN     38       1.0544       0.027178   \n",
      "39           NaN                      NaN     39       1.0864       0.027190   \n",
      "40           NaN                      NaN     40       1.1184       0.027209   \n",
      "41      0.026362                    0.604     41       1.1184            NaN   \n",
      "42           NaN                      NaN     42       1.1184            NaN   \n",
      "\n",
      "    eval/steps_per_second  train/loss    _runtime  eval/runtime  \\\n",
      "0                     NaN      2.1611  786.563118           NaN   \n",
      "1                     NaN      2.1598  786.563118           NaN   \n",
      "2                     NaN      2.1558  786.563118           NaN   \n",
      "3                     NaN      2.1475  786.563118           NaN   \n",
      "4                     NaN      2.1315  786.563118           NaN   \n",
      "5                   0.076         NaN  786.563118      824.5038   \n",
      "6                     NaN      2.1021  786.563118           NaN   \n",
      "7                     NaN      2.0671  786.563118           NaN   \n",
      "8                     NaN      2.0409  786.563118           NaN   \n",
      "9                     NaN      2.0135  786.563118           NaN   \n",
      "10                    NaN      1.9718  786.563118           NaN   \n",
      "11                  0.077         NaN  786.563118      819.3431   \n",
      "12                    NaN      1.9188  786.563118           NaN   \n",
      "13                    NaN      1.8527  786.563118           NaN   \n",
      "14                    NaN      1.7712  786.563118           NaN   \n",
      "15                    NaN      1.6675  786.563118           NaN   \n",
      "16                    NaN      1.5434  786.563118           NaN   \n",
      "17                  0.077         NaN  786.563118      820.0341   \n",
      "18                    NaN      1.3989  786.563118           NaN   \n",
      "19                    NaN      1.2289  786.563118           NaN   \n",
      "20                    NaN      1.0360  786.563118           NaN   \n",
      "21                    NaN      0.8215  786.563118           NaN   \n",
      "22                    NaN      0.5826  786.563118           NaN   \n",
      "23                  0.077         NaN  786.563118      820.2847   \n",
      "24                    NaN      0.3598  786.563118           NaN   \n",
      "25                    NaN      0.1819  786.563118           NaN   \n",
      "26                    NaN      0.0666  786.563118           NaN   \n",
      "27                    NaN      0.0185  786.563118           NaN   \n",
      "28                    NaN      0.0065  786.563118           NaN   \n",
      "29                  0.077         NaN  786.563118      820.5675   \n",
      "30                    NaN      0.0045  786.563118           NaN   \n",
      "31                    NaN      0.0041  786.563118           NaN   \n",
      "32                    NaN      0.0039  786.563118           NaN   \n",
      "33                    NaN      0.0038  786.563118           NaN   \n",
      "34                    NaN      0.0037  786.563118           NaN   \n",
      "35                  0.076         NaN  786.563118      827.0299   \n",
      "36                    NaN      0.0037  786.563118           NaN   \n",
      "37                    NaN      0.0037  786.563118           NaN   \n",
      "38                    NaN      0.0036  786.563118           NaN   \n",
      "39                    NaN      0.0036  786.563118           NaN   \n",
      "40                    NaN      0.0036  786.563118           NaN   \n",
      "41                  0.076         NaN  786.563118      827.7177   \n",
      "42                    NaN         NaN  786.563118           NaN   \n",
      "\n",
      "    train/grad_norm  train/learning_rate  eval/loss    _timestamp  \\\n",
      "0          0.960938         9.574468e-07        NaN  1.768229e+09   \n",
      "1          0.972656         2.021277e-06        NaN  1.768230e+09   \n",
      "2          0.957031         3.085106e-06        NaN  1.768231e+09   \n",
      "3          0.976562         4.148936e-06        NaN  1.768232e+09   \n",
      "4          0.960938         5.212766e-06        NaN  1.768234e+09   \n",
      "5               NaN                  NaN   2.115630  1.768234e+09   \n",
      "6          0.789062         6.276596e-06        NaN  1.768236e+09   \n",
      "7          0.349609         7.340426e-06        NaN  1.768237e+09   \n",
      "8          0.287109         8.404255e-06        NaN  1.768238e+09   \n",
      "9          0.283203         9.468085e-06        NaN  1.768239e+09   \n",
      "10         0.300781         9.999136e-06        NaN  1.768241e+09   \n",
      "11              NaN                  NaN   1.944338  1.768241e+09   \n",
      "12         0.333984         9.992227e-06        NaN  1.768243e+09   \n",
      "13         0.388672         9.978418e-06        NaN  1.768244e+09   \n",
      "14         0.468750         9.957728e-06        NaN  1.768245e+09   \n",
      "15         0.558594         9.930187e-06        NaN  1.768246e+09   \n",
      "16         0.656250         9.895831e-06        NaN  1.768247e+09   \n",
      "17              NaN                  NaN   1.468676  1.768248e+09   \n",
      "18         0.808594         9.854709e-06        NaN  1.768249e+09   \n",
      "19         0.945312         9.806877e-06        NaN  1.768251e+09   \n",
      "20         1.101562         9.752402e-06        NaN  1.768252e+09   \n",
      "21         1.328125         9.691359e-06        NaN  1.768253e+09   \n",
      "22         1.320312         9.623831e-06        NaN  1.768254e+09   \n",
      "23              NaN                  NaN   0.451372  1.768255e+09   \n",
      "24         1.187500         9.549913e-06        NaN  1.768256e+09   \n",
      "25         0.824219         9.469707e-06        NaN  1.768257e+09   \n",
      "26         0.455078         9.383323e-06        NaN  1.768258e+09   \n",
      "27         0.179688         9.290881e-06        NaN  1.768260e+09   \n",
      "28         0.056641         9.192509e-06        NaN  1.768261e+09   \n",
      "29              NaN                  NaN   0.004920  1.768262e+09   \n",
      "30         0.038574         9.088342e-06        NaN  1.768263e+09   \n",
      "31         0.028564         8.978525e-06        NaN  1.768264e+09   \n",
      "32         0.024780         8.863209e-06        NaN  1.768265e+09   \n",
      "33         0.024902         8.742554e-06        NaN  1.768266e+09   \n",
      "34         0.023438         8.616726e-06        NaN  1.768268e+09   \n",
      "35              NaN                  NaN   0.003593  1.768268e+09   \n",
      "36         0.021484         8.485900e-06        NaN  1.768270e+09   \n",
      "37         0.020996         8.350255e-06        NaN  1.768271e+09   \n",
      "38         0.020630         8.209981e-06        NaN  1.768272e+09   \n",
      "39         0.020752         8.065270e-06        NaN  1.768273e+09   \n",
      "40         0.019409         7.916322e-06        NaN  1.768274e+09   \n",
      "41              NaN                  NaN   0.003484  1.768275e+09   \n",
      "42              NaN                  NaN        NaN  1.768275e+09   \n",
      "\n",
      "    train/mean_token_accuracy  train/num_tokens  train/global_step  \\\n",
      "0                    0.569618           81920.0                 10   \n",
      "1                    0.568897          163840.0                 20   \n",
      "2                    0.569313          245760.0                 30   \n",
      "3                    0.569129          327680.0                 40   \n",
      "4                    0.572187          409600.0                 50   \n",
      "5                         NaN               NaN                 50   \n",
      "6                    0.573716          491520.0                 60   \n",
      "7                    0.573141          573440.0                 70   \n",
      "8                    0.571404          655360.0                 80   \n",
      "9                    0.570780          737280.0                 90   \n",
      "10                   0.576492          819200.0                100   \n",
      "11                        NaN               NaN                100   \n",
      "12                   0.583867          901120.0                110   \n",
      "13                   0.595682          983040.0                120   \n",
      "14                   0.608207         1064960.0                130   \n",
      "15                   0.614298         1146880.0                140   \n",
      "16                   0.618334         1228800.0                150   \n",
      "17                        NaN               NaN                150   \n",
      "18                   0.642882         1310720.0                160   \n",
      "19                   0.690607         1392640.0                170   \n",
      "20                   0.745536         1474560.0                180   \n",
      "21                   0.800954         1556480.0                190   \n",
      "22                   0.859283         1638400.0                200   \n",
      "23                        NaN               NaN                200   \n",
      "24                   0.914934         1720320.0                210   \n",
      "25                   0.962133         1802240.0                220   \n",
      "26                   0.987378         1884160.0                230   \n",
      "27                   0.997713         1966080.0                240   \n",
      "28                   1.000000         2048000.0                250   \n",
      "29                        NaN               NaN                250   \n",
      "30                   1.000000         2129920.0                260   \n",
      "31                   1.000000         2211840.0                270   \n",
      "32                   1.000000         2293760.0                280   \n",
      "33                   1.000000         2375680.0                290   \n",
      "34                   1.000000         2457600.0                300   \n",
      "35                        NaN               NaN                300   \n",
      "36                   1.000000         2539520.0                310   \n",
      "37                   1.000000         2617344.0                320   \n",
      "38                   1.000000         2699264.0                330   \n",
      "39                   1.000000         2781184.0                340   \n",
      "40                   1.000000         2863104.0                350   \n",
      "41                        NaN               NaN                350   \n",
      "42                        NaN               NaN                350   \n",
      "\n",
      "    eval/mean_token_accuracy  eval/num_tokens  \n",
      "0                        NaN              NaN  \n",
      "1                        NaN              NaN  \n",
      "2                        NaN              NaN  \n",
      "3                        NaN              NaN  \n",
      "4                        NaN              NaN  \n",
      "5                   0.577299         409600.0  \n",
      "6                        NaN              NaN  \n",
      "7                        NaN              NaN  \n",
      "8                        NaN              NaN  \n",
      "9                        NaN              NaN  \n",
      "10                       NaN              NaN  \n",
      "11                  0.587022         819200.0  \n",
      "12                       NaN              NaN  \n",
      "13                       NaN              NaN  \n",
      "14                       NaN              NaN  \n",
      "15                       NaN              NaN  \n",
      "16                       NaN              NaN  \n",
      "17                  0.628149        1228800.0  \n",
      "18                       NaN              NaN  \n",
      "19                       NaN              NaN  \n",
      "20                       NaN              NaN  \n",
      "21                       NaN              NaN  \n",
      "22                       NaN              NaN  \n",
      "23                  0.892368        1638400.0  \n",
      "24                       NaN              NaN  \n",
      "25                       NaN              NaN  \n",
      "26                       NaN              NaN  \n",
      "27                       NaN              NaN  \n",
      "28                       NaN              NaN  \n",
      "29                  1.000000        2048000.0  \n",
      "30                       NaN              NaN  \n",
      "31                       NaN              NaN  \n",
      "32                       NaN              NaN  \n",
      "33                       NaN              NaN  \n",
      "34                       NaN              NaN  \n",
      "35                  1.000000        2457600.0  \n",
      "36                       NaN              NaN  \n",
      "37                       NaN              NaN  \n",
      "38                       NaN              NaN  \n",
      "39                       NaN              NaN  \n",
      "40                       NaN              NaN  \n",
      "41                  1.000000        2863104.0  \n",
      "42                       NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(\"/t-p-angevare-university-of-twente/Qwen-fine-tuning/runs/27s35zzj\")\n",
    "history = run.history()\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for inference...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f41f5c108c497188514d63559e100e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned LoRA adapters...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import json\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"Loading base model for inference...\")\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Loading fine-tuned LoRA adapters...\")\n",
    "inference_model = PeftModel.from_pretrained(inference_model, \"./sft_14b_output/final_model\")\n",
    "inference_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yhw49tyyn3k",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_entities(text, max_new_tokens=512):\n",
    "    \"\"\"Run entity extraction on input text using the fine-tuned R1-Distill model.\"\"\"\n",
    "    \n",
    "    # Format as chat message - R1-Distill: no system prompt\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt + \"\\n\\nInput text:\\n\" + text}\n",
    "    ]\n",
    "    \n",
    "    # Tokenize\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(inference_model.device)\n",
    "    \n",
    "    # Generate with recommended R1-Distill settings\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,  # Recommended for R1-Distill (0.5-0.7)\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Strip thinking tokens before parsing JSON\n",
    "    response_clean = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        # Find JSON in response\n",
    "        start = response_clean.find('{')\n",
    "        end = response_clean.rfind('}') + 1\n",
    "        if start != -1 and end > start:\n",
    "            result = json.loads(response_clean[start:end])\n",
    "            return result, response\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    return None, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohw1yr4zvth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING FINE-TUNED MODEL\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TEST 1\n",
      "================================================================================\n",
      "INPUT: Please contact John Smith at john.smith@company.com. The server IP is 192.168.1.100.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2532: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTED ENTITIES (2 found):\n",
      "  - EMAIL: john.smith@company.com\n",
      "  - IP: 192.168.1.100\n",
      "\n",
      "================================================================================\n",
      "TEST 2\n",
      "================================================================================\n",
      "INPUT: Maria Garcia traveled from Madrid to New York last week.\n",
      "----------------------------------------\n",
      "EXTRACTED ENTITIES (3 found):\n",
      "  - PERSON: Maria Garcia\n",
      "  - LOCATION: Madrid\n",
      "  - LOCATION: New York\n",
      "\n",
      "================================================================================\n",
      "TEST 3\n",
      "================================================================================\n",
      "INPUT: Call me at +1-555-123-4567 or email support@example.org\n",
      "----------------------------------------\n",
      "EXTRACTED ENTITIES (2 found):\n",
      "  - PHONE: +1-555-123-4567\n",
      "  - EMAIL: support@example.org\n",
      "\n",
      "================================================================================\n",
      "TEST 4\n",
      "================================================================================\n",
      "INPUT: The suspect known as darkh4cker_99 was traced to IP 10.0.0.1 in Berlin, Germany.\n",
      "----------------------------------------\n",
      "EXTRACTED ENTITIES (3 found):\n",
      "  - PERSON: darkh4cker_99\n",
      "  - IP: 10.0.0.1\n",
      "  - LOCATION: Berlin\n",
      "\n",
      "================================================================================\n",
      "TESTING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test on various examples\n",
    "test_cases = [\n",
    "    # Test 1: Email and IP\n",
    "    \"Please contact John Smith at john.smith@company.com. The server IP is 192.168.1.100.\",\n",
    "    \n",
    "    # Test 2: Location and person  \n",
    "    \"Maria Garcia traveled from Madrid to New York last week.\",\n",
    "    \n",
    "    # Test 3: Phone number\n",
    "    \"Call me at +1-555-123-4567 or email support@example.org\",\n",
    "    \n",
    "    # Test 4: Mixed entities (username)\n",
    "    \"The suspect known as darkh4cker_99 was traced to IP 10.0.0.1 in Berlin, Germany.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_text in enumerate(test_cases):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"INPUT: {test_text[:200]}{'...' if len(test_text) > 200 else ''}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result, raw_response = extract_entities(test_text)\n",
    "    \n",
    "    if result and 'entities' in result:\n",
    "        print(f\"EXTRACTED ENTITIES ({len(result['entities'])} found):\")\n",
    "        for ent in result['entities']:\n",
    "            print(f\"  - {ent.get('type', 'UNKNOWN')}: {ent.get('entity', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"RAW RESPONSE: {raw_response[:500]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3qhfxe7n4i7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with ground truth from validation set\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON WITH GROUND TRUTH (Validation Sample)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample = val[0]\n",
    "print(f\"\\nINPUT TEXT:\\n{sample['source_text'][:300]}...\")\n",
    "\n",
    "# Get model prediction\n",
    "result, _ = extract_entities(sample['source_text'])\n",
    "\n",
    "\n",
    "ground_truth = sample['privacy_mask']\n",
    "\n",
    "print(f\"\\n{'GROUND TRUTH':-^40}\")\n",
    "for ent in ground_truth[:10]: \n",
    "    print(f\"  - {ent['type']}: {ent['text']}\")\n",
    "if len(ground_truth) > 10:\n",
    "    print(f\"  ... and {len(ground_truth) - 10} more\")\n",
    "\n",
    "print(f\"\\n{'MODEL PREDICTION':-^40}\")\n",
    "if result and 'entities' in result:\n",
    "    for ent in result['entities'][:10]:\n",
    "        print(f\"  - {ent.get('type', 'UNKNOWN')}: {ent.get('entity', 'N/A')}\")\n",
    "    if len(result['entities']) > 10:\n",
    "        print(f\"  ... and {len(result['entities']) - 10} more\")\n",
    "else:\n",
    "    print(\"  No entities extracted or invalid JSON response\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
