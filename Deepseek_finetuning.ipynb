{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3f6161",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 09:29:21.938693: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-06 09:29:21.981676: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-06 09:29:21.981708: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-06 09:29:21.981762: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-06 09:29:21.992416: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-06 09:29:23.809278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import gc\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe343c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mt-p-angevare\u001b[0m (\u001b[33mt-p-angevare-university-of-twente\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20260106_092959-uf8ezp4n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/t-p-angevare-university-of-twente/deepseek-fine-tuning/runs/uf8ezp4n' target=\"_blank\">14b-ioc-extraction</a></strong> to <a href='https://wandb.ai/t-p-angevare-university-of-twente/deepseek-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/t-p-angevare-university-of-twente/deepseek-fine-tuning' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/deepseek-fine-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/t-p-angevare-university-of-twente/deepseek-fine-tuning/runs/uf8ezp4n' target=\"_blank\">https://wandb.ai/t-p-angevare-university-of-twente/deepseek-fine-tuning/runs/uf8ezp4n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/t-p-angevare-university-of-twente/deepseek-fine-tuning/runs/uf8ezp4n?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5d13f25fc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"deepseek-fine-tuning\", name=\"14b-ioc-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360cf854",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97352ee1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 29908\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source_text', 'privacy_mask', 'id'],\n",
       "        num_rows: 7946\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"ai4privacy/pii-masking-300k\")\n",
    "dataset = dataset.filter(lambda x: x['language'] == 'English')\n",
    "dataset = dataset.select_columns([\"source_text\", \"privacy_mask\", \"id\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb6ee38",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "dataset_entity_mapping = {\n",
    "    'USERNAME' : 'USERNAME',\n",
    "    'EMAIL' : 'EMAIL',\n",
    "    'LASTNAME1' : 'PERSON',\n",
    "    'IP' : 'IP',\n",
    "    'GIVENNAME1' : 'PERSON',\n",
    "    'TEL' : 'PHONE',\n",
    "    'CITY' : 'LOCATION',\n",
    "    'POSTCODE' : 'LOCATION',\n",
    "    'STREET': 'LOCATION',\n",
    "    'STATE' : 'LOCATION',\n",
    "    'BUILDING' : 'LOCATION',\n",
    "    'COUNTRY' : 'LOCATION',\n",
    "    'SECADDRESS' : 'LOCATION',\n",
    "    'LASTNAME2' : 'PERSON',\n",
    "    'GIVENNAME2' : 'PERSON',\n",
    "    'GEOCOORD' : 'LOCATION',\n",
    "    'LASTNAME3' : 'PERSON'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04564bee",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def clean_entities(privacy_mask):\n",
    "    new_entities = []\n",
    "    for ent in privacy_mask:\n",
    "        if ent['label'] in dataset_entity_mapping.keys():\n",
    "            new_entities.append({\n",
    "                'type': dataset_entity_mapping.get(ent['label']),\n",
    "                'text' : ent['value'],\n",
    "                'start_pos' : ent['start'],\n",
    "                'end_pos' : ent['end']\n",
    "            })\n",
    "    return new_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f4e32b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {'privacy_mask': clean_entities(x['privacy_mask'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e319409",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': 'Subject: Group Messaging for Admissions Process\\n\\nGood morning, everyone,\\n\\nI hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\\n\\n- wynqvrh053 - Meeting at 10:20am\\n- luka.burg - Meeting at 21\\n- qahil.wittauer - Meeting at quarter past 13\\n- gholamhossein.ruschke - Meeting at 9:47 PM\\n- pdmjrsyoz1460 ',\n",
       " 'privacy_mask': [{'end_pos': 297,\n",
       "   'start_pos': 287,\n",
       "   'text': 'wynqvrh053',\n",
       "   'type': 'USERNAME'},\n",
       "  {'end_pos': 330, 'start_pos': 321, 'text': 'luka.burg', 'type': 'USERNAME'},\n",
       "  {'end_pos': 363,\n",
       "   'start_pos': 349,\n",
       "   'text': 'qahil.wittauer',\n",
       "   'type': 'USERNAME'},\n",
       "  {'end_pos': 416,\n",
       "   'start_pos': 395,\n",
       "   'text': 'gholamhossein.ruschke',\n",
       "   'type': 'USERNAME'},\n",
       "  {'end_pos': 453,\n",
       "   'start_pos': 440,\n",
       "   'text': 'pdmjrsyoz1460',\n",
       "   'type': 'USERNAME'}],\n",
       " 'id': '40767A'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56659c8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a cyber intelligence analyst with 20 years of experience in the the field.\n",
    "\n",
    "Your task is to extract any entity from the input text. For each entity found you MUST indicate the type in UPPERCASE. ONLY extract entities if literal entity is present in input text.\n",
    "The expected entity types are the following:\n",
    "\n",
    "- EMAIL: email addresses format (user@domain.tld)\n",
    "- IP: IP addresses (IPv4 x.x.x.x or IPv6)\n",
    "- BTC: ONLY Bitcoin wallet addresses (26-35 alphanumeric, starting with 1, 3, or bc1) EXCLUDE the word bitcoin or values (for example 2.0 BTC)\n",
    "- IBAN: iban bank account number\n",
    "- PERSON: Human names (John Smith, John, Catalina) EXCLUDE initials (for example A.H.) \n",
    "- LOCATION: cities, countries, geographic locations, regions\n",
    "- PHONE: phone numbers in any format\n",
    "- WEB_RESOURCE: URLs and web addresses EXCLUDE filenames\n",
    " \n",
    "**Output**:\n",
    "The output MUST be in a JSON object with key 'entities' and the value a list of dictionaries including every entity found. For each entity you MUST indicate the type in UPPERCASE.\n",
    "\n",
    "**OUTPUT EXAMPLE**:\n",
    "{\n",
    "  \"entities\": [\n",
    "    {\"entity\": \"target123@darkmail.org\", \"type\": \"EMAIL\"},\n",
    "    {\"entity\": \"10.45.67.89\", \"type\": \"IP\"},\n",
    "    {\"entity\": \"Thompson\", \"type\": \"PERSON\"},\n",
    "    {\"entity\": \"Helsinki\", \"type\": \"LOCATION\"},\n",
    "    {\"entity\": \"Tim\", \"type\": \"PERSON\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "Return empty array if no entities found in the input text.\n",
    "PAY ATTENTION to sentences that begin with entity type PERSON, for example Anna.\n",
    "PAY ATTENTION to when the sentences begin with possesive forms of entity type PERSON, for example Catalina's\n",
    "PAY ATTENTION to when the sentences contain a FULL NAME, the FULL NAME MUST be extracted as ONE entity.\n",
    "DO NOT include any entities from the example or the system prompt in your answer.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d907eb0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_chatml(source_text, privacy_mask):\n",
    "    \"\"\"Convert training data to ChatML format with thinking tokens for R1-Distill models.\n",
    "    \n",
    "    Note: R1-Distill models don't use system prompts - all instructions go in user message.\n",
    "    \"\"\"\n",
    "    # Convert to the JSON format expected by the prompt\n",
    "    entities_json = {\n",
    "        \"entities\": [\n",
    "            {\"entity\": ent['text'], \"type\": ent['type']} \n",
    "            for ent in privacy_mask\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Build reasoning trace with thinking tokens\n",
    "    thinking = \"<think>\\nI need to extract entities from this text.\\n\"\n",
    "    for ent in privacy_mask:\n",
    "        thinking += f\"Found {ent['type']}: \\\"{ent['text']}\\\"\\n\"\n",
    "    thinking += \"</think>\\n\\n\"\n",
    "    \n",
    "    # R1-Distill: No system prompt, instructions in user message\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": prompt + \"\\n\\nInput text:\\n\" + source_text},\n",
    "        {\"role\": \"assistant\", \"content\": thinking + json.dumps(entities_json, indent=2)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2936c99",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = dataset.map(lambda x: {\"messages\": convert_to_chatml(x['source_text'], x['privacy_mask'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17bde313",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 5000\n",
      "Validation samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Reduced dataset for faster training (~2-3 hours instead of 12+)\n",
    "train = dataset['train'].select(range(5000))  # 5k samples (was 30k)\n",
    "val = dataset['validation'].select(range(500))\n",
    "\n",
    "print(f\"Training samples: {len(train)}\")\n",
    "print(f\"Validation samples: {len(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a0ee2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Source Text (truncated) ===\n",
      "Subject: Group Messaging for Admissions Process\n",
      "\n",
      "Good morning, everyone,\n",
      "\n",
      "I hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developm...\n",
      "\n",
      "=== User Message (prompt + input) ===\n",
      "\n",
      "You are a cyber intelligence analyst with 20 years of experience in the the field.\n",
      "\n",
      "Your task is to extract any entity from the input text. For each entity found you MUST indicate the type in UPPERCASE. ONLY extract entities if literal entity is present in input text.\n",
      "The expected entity types are ...\n",
      "\n",
      "=== Assistant Response (with thinking tokens) ===\n",
      "<think>\n",
      "I need to extract entities from this text.\n",
      "Found USERNAME: \"wynqvrh053\"\n",
      "Found USERNAME: \"luka.burg\"\n",
      "Found USERNAME: \"qahil.wittauer\"\n",
      "Found USERNAME: \"gholamhossein.ruschke\"\n",
      "Found USERNAME: \"pdmjrsyoz1460\"\n",
      "</think>\n",
      "\n",
      "{\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"entity\": \"wynqvrh053\",\n",
      "      \"type\": \"USERNAME\"\n",
      "    },\n",
      "    {\n",
      "      \"entity\": \"luka.burg\",\n",
      "      \"type\": \"USERNAME\"\n",
      "    },\n",
      "    {\n",
      "      \"entity\": \"qahil.wittauer\",\n",
      "      \"type\": \"USERNAME\"\n",
      "    },\n",
      "    {\n",
      "      \"entity\": \"gholamhossein.ruschke\",\n",
      "     ...\n",
      "\n",
      "=== Format: <think>reasoning</think> + JSON output ===\n"
     ]
    }
   ],
   "source": [
    "# verify format with thinking tokens\n",
    "example = train[0]\n",
    "print(\"=== Source Text (truncated) ===\")\n",
    "print(example['source_text'][:200] + \"...\")\n",
    "print(\"\\n=== User Message (prompt + input) ===\")\n",
    "print(example['messages'][0]['content'][:300] + \"...\")\n",
    "print(\"\\n=== Assistant Response (with thinking tokens) ===\")\n",
    "print(example['messages'][1]['content'][:500] + \"...\")\n",
    "print(\"\\n=== Format: <think>reasoning</think> + JSON output ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b196bc8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf51c16d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278b7eef15674a219f37d38128cfb6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e195b0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA config: r=8, alpha=16, dropout=0.1\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                   \n",
    "    lora_alpha=16,           \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7b5b7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21b5aff58774c63af400b296afacc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dff1483d2bf4c2195b5e0fcdc9513a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration for DeepSeek-R1-Distill-Qwen-14B:\n",
      "  - Train samples: 5000\n",
      "  - Epochs: 3\n",
      "  - Effective batch size: 16\n",
      "  - Learning rate: 1e-05\n",
      "  - Eval every 50 steps\n",
      "  - Max grad norm: 0.5\n",
      "  - Early stopping patience: 2 evals\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_14b_output\",\n",
    "\n",
    "    num_train_epochs=3,                 \n",
    "    \n",
    "    max_length=512,\n",
    "    per_device_train_batch_size=1,       \n",
    "    gradient_accumulation_steps=16,      \n",
    "    \n",
    "\n",
    "    learning_rate=1e-5,                  \n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=0.5,                   \n",
    "    \n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,                      \n",
    "    \n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "\n",
    "    packing=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"deepseek-r1-14b-pii\",\n",
    "    bf16=True,\n",
    "    optim=\"adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    peft_config=lora_config,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=2,\n",
    "            early_stopping_threshold=0.005\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Training configuration for DeepSeek-R1-Distill-Qwen-14B:\")\n",
    "print(f\"  - Train samples: {len(train)}\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Eval every {training_args.eval_steps} steps\")\n",
    "print(f\"  - Max grad norm: {training_args.max_grad_norm}\")\n",
    "print(f\"  - Early stopping patience: 2 evals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60ea7b8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 151646, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18/939 31:27 < 30:10:28, 0.01 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:1245\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:1175\u001b[0m, in \u001b[0;36mSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1174\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition_ids\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1175\u001b[0m         entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_for_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentropy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics[mode][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(entropy)\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2417ab2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./sft_output/final_model\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./sft_14b_output/final_model\")\n",
    "print(\"Model saved to ./sft_14b_output/final_model\")\n",
    "\n",
    "# Log final metrics\n",
    "if wandb.run:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00623c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    eval/loss  train/entropy  train/epoch  eval/entropy  \\\n",
      "0         NaN       1.284441        0.016           NaN   \n",
      "1         NaN       1.269152        0.032           NaN   \n",
      "2         NaN       1.277675        0.048           NaN   \n",
      "3         NaN       1.298521        0.064           NaN   \n",
      "4         NaN       1.296141        0.080           NaN   \n",
      "..        ...            ...          ...           ...   \n",
      "56        NaN       0.450221        0.768           NaN   \n",
      "57        NaN       0.438981        0.784           NaN   \n",
      "58        NaN       0.450000        0.800           NaN   \n",
      "59   0.418354            NaN        0.800       0.43036   \n",
      "60        NaN            NaN        0.800           NaN   \n",
      "\n",
      "    eval/steps_per_second    _timestamp  eval/num_tokens  train/num_tokens  \\\n",
      "0                     NaN  1.767459e+09              NaN          129032.0   \n",
      "1                     NaN  1.767460e+09              NaN          257738.0   \n",
      "2                     NaN  1.767461e+09              NaN          386236.0   \n",
      "3                     NaN  1.767462e+09              NaN          511714.0   \n",
      "4                     NaN  1.767462e+09              NaN          638004.0   \n",
      "..                    ...           ...              ...               ...   \n",
      "56                    NaN  1.767520e+09              NaN         6161836.0   \n",
      "57                    NaN  1.767521e+09              NaN         6289116.0   \n",
      "58                    NaN  1.767522e+09              NaN         6416551.0   \n",
      "59                  0.073  1.767524e+09        6416551.0               NaN   \n",
      "60                    NaN  1.767524e+09              NaN               NaN   \n",
      "\n",
      "    train/global_step  _step  eval/runtime  train/grad_norm  \\\n",
      "0                  10      0           NaN         0.251953   \n",
      "1                  20      1           NaN         0.263672   \n",
      "2                  30      2           NaN         0.281250   \n",
      "3                  40      3           NaN         0.277344   \n",
      "4                  50      4           NaN         0.296875   \n",
      "..                ...    ...           ...              ...   \n",
      "56                480     56           NaN         0.106445   \n",
      "57                490     57           NaN         0.108398   \n",
      "58                500     58           NaN         0.094238   \n",
      "59                500     59       1703.61              NaN   \n",
      "60                500     60           NaN              NaN   \n",
      "\n",
      "    train/learning_rate  train/mean_token_accuracy  train/loss  \\\n",
      "0          4.787234e-07                   0.634307      1.8155   \n",
      "1          1.010638e-06                   0.638439      1.7967   \n",
      "2          1.542553e-06                   0.635597      1.7983   \n",
      "3          2.074468e-06                   0.632969      1.8128   \n",
      "4          2.606383e-06                   0.635534      1.8003   \n",
      "..                  ...                        ...         ...   \n",
      "56         9.283624e-06                   0.904083      0.4442   \n",
      "57         9.234859e-06                   0.907369      0.4297   \n",
      "58         9.184626e-06                   0.904908      0.4402   \n",
      "59                  NaN                        NaN         NaN   \n",
      "60                  NaN                        NaN         NaN   \n",
      "\n",
      "    eval/samples_per_second  _runtime  eval/mean_token_accuracy  \n",
      "0                       NaN   4.77658                       NaN  \n",
      "1                       NaN   4.77658                       NaN  \n",
      "2                       NaN   4.77658                       NaN  \n",
      "3                       NaN   4.77658                       NaN  \n",
      "4                       NaN   4.77658                       NaN  \n",
      "..                      ...       ...                       ...  \n",
      "56                      NaN   4.77658                       NaN  \n",
      "57                      NaN   4.77658                       NaN  \n",
      "58                      NaN   4.77658                       NaN  \n",
      "59                    0.587   4.77658                  0.909308  \n",
      "60                      NaN   4.77658                       NaN  \n",
      "\n",
      "[61 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(\"/t-p-angevare-university-of-twente/transformer-fine-tuning/runs/rjx3lp23\")\n",
    "history = run.history()\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498a876",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model for inference...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f41f5c108c497188514d63559e100e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned LoRA adapters...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import json\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"Loading base model for inference...\")\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Loading fine-tuned LoRA adapters...\")\n",
    "inference_model = PeftModel.from_pretrained(inference_model, \"./sft_14b_output/final_model\")\n",
    "inference_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yhw49tyyn3k",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_entities(text, max_new_tokens=512):\n",
    "    \"\"\"Run entity extraction on input text using the fine-tuned R1-Distill model.\"\"\"\n",
    "    \n",
    "    # Format as chat message - R1-Distill: no system prompt\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt + \"\\n\\nInput text:\\n\" + text}\n",
    "    ]\n",
    "    \n",
    "    # Tokenize\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(inference_model.device)\n",
    "    \n",
    "    # Generate with recommended R1-Distill settings\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,  # Recommended for R1-Distill (0.5-0.7)\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Strip thinking tokens before parsing JSON\n",
    "    response_clean = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "    \n",
    "    # Try to parse as JSON\n",
    "    try:\n",
    "        # Find JSON in response\n",
    "        start = response_clean.find('{')\n",
    "        end = response_clean.rfind('}') + 1\n",
    "        if start != -1 and end > start:\n",
    "            result = json.loads(response_clean[start:end])\n",
    "            return result, response\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    return None, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ohw1yr4zvth",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING FINE-TUNED MODEL\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TEST 1\n",
      "================================================================================\n",
      "INPUT: Please contact John Smith at john.smith@company.com. The server IP is 192.168.1.100.\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2532: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTED ENTITIES (2 found):\n",
      "  - EMAIL: john.smith@company.com\n",
      "  - IP: 192.168.1.100\n",
      "\n",
      "================================================================================\n",
      "TEST 2\n",
      "================================================================================\n",
      "INPUT: Maria Garcia traveled from Madrid to New York last week.\n",
      "----------------------------------------\n",
      "EXTRACTED ENTITIES (3 found):\n",
      "  - PERSON: Maria Garcia\n",
      "  - LOCATION: Madrid\n",
      "  - LOCATION: New York\n",
      "\n",
      "================================================================================\n",
      "TEST 3\n",
      "================================================================================\n",
      "INPUT: Call me at +1-555-123-4567 or email support@example.org\n",
      "----------------------------------------\n",
      "EXTRACTED ENTITIES (2 found):\n",
      "  - PHONE: +1-555-123-4567\n",
      "  - EMAIL: support@example.org\n",
      "\n",
      "================================================================================\n",
      "TEST 4\n",
      "================================================================================\n",
      "INPUT: The suspect known as darkh4cker_99 was traced to IP 10.0.0.1 in Berlin, Germany.\n",
      "----------------------------------------\n",
      "EXTRACTED ENTITIES (3 found):\n",
      "  - PERSON: darkh4cker_99\n",
      "  - IP: 10.0.0.1\n",
      "  - LOCATION: Berlin\n",
      "\n",
      "================================================================================\n",
      "TESTING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test on various examples\n",
    "test_cases = [\n",
    "    # Test 1: Email and IP\n",
    "    \"Please contact John Smith at john.smith@company.com. The server IP is 192.168.1.100.\",\n",
    "    \n",
    "    # Test 2: Location and person  \n",
    "    \"Maria Garcia traveled from Madrid to New York last week.\",\n",
    "    \n",
    "    # Test 3: Phone number\n",
    "    \"Call me at +1-555-123-4567 or email support@example.org\",\n",
    "    \n",
    "    # Test 4: Mixed entities (username)\n",
    "    \"The suspect known as darkh4cker_99 was traced to IP 10.0.0.1 in Berlin, Germany.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING FINE-TUNED MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_text in enumerate(test_cases):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"INPUT: {test_text[:200]}{'...' if len(test_text) > 200 else ''}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result, raw_response = extract_entities(test_text)\n",
    "    \n",
    "    if result and 'entities' in result:\n",
    "        print(f\"EXTRACTED ENTITIES ({len(result['entities'])} found):\")\n",
    "        for ent in result['entities']:\n",
    "            print(f\"  - {ent.get('type', 'UNKNOWN')}: {ent.get('entity', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"RAW RESPONSE: {raw_response[:500]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3qhfxe7n4i7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Compare with ground truth from validation set\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON WITH GROUND TRUTH (Validation Sample)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample = val[0]\n",
    "print(f\"\\nINPUT TEXT:\\n{sample['source_text'][:300]}...\")\n",
    "\n",
    "# Get model prediction\n",
    "result, _ = extract_entities(sample['source_text'])\n",
    "\n",
    "\n",
    "ground_truth = sample['privacy_mask']\n",
    "\n",
    "print(f\"\\n{'GROUND TRUTH':-^40}\")\n",
    "for ent in ground_truth[:10]: \n",
    "    print(f\"  - {ent['type']}: {ent['text']}\")\n",
    "if len(ground_truth) > 10:\n",
    "    print(f\"  ... and {len(ground_truth) - 10} more\")\n",
    "\n",
    "print(f\"\\n{'MODEL PREDICTION':-^40}\")\n",
    "if result and 'entities' in result:\n",
    "    for ent in result['entities'][:10]:\n",
    "        print(f\"  - {ent.get('type', 'UNKNOWN')}: {ent.get('entity', 'N/A')}\")\n",
    "    if len(result['entities']) > 10:\n",
    "        print(f\"  ... and {len(result['entities']) - 10} more\")\n",
    "else:\n",
    "    print(\"  No entities extracted or invalid JSON response\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
